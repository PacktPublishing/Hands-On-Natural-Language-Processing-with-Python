{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Machine Translation for French to English</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "3aee62b9-47ce-e416-5816-8df7126fe690"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i346047/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/i346047/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gzip\n",
    "import codecs\n",
    "import re\n",
    "import time\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib.seq2seq import TrainingHelper, GreedyEmbeddingHelper, BasicDecoder, dynamic_decode\n",
    "from tensorflow.contrib.seq2seq import BahdanauAttention, AttentionWrapper, sequence_loss\n",
    "from tensorflow.contrib.rnn import GRUCell, DropoutWrapper\n",
    "TOKEN_GO = '<GO>'\n",
    "TOKEN_EOS = '<EOS>'\n",
    "TOKEN_PAD = '<PAD>'\n",
    "TOKEN_UNK = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "654175ff-07c7-455d-1b8f-d702cee211c4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frdata=[]\n",
    "endata=[]\n",
    "with open('data/train_fr_lines.txt') as frfile:\n",
    "    for li in frfile:\n",
    "        frdata.append(li)\n",
    "with open('data/train_en_lines.txt') as enfile:\n",
    "    for li in enfile:\n",
    "        endata.append(li)\n",
    "mtdata = pd.DataFrame({'FR':frdata,'EN':endata})\n",
    "mtdata['FR_len'] = mtdata['FR'].apply(lambda x: len(x.split(' ')))\n",
    "mtdata['EN_len'] = mtdata['EN'].apply(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Voici Bill Lange. Je suis Dave Gallo.\\n'\n",
      " 'Nous allons vous raconter quelques histoires de la mer en vidéo.\\n']\n",
      "[\"This is Bill Lange. I'm Dave Gallo.\\n\"\n",
      " \"And we're going to tell you some stories from the sea here in video.\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(mtdata['FR'].head(2).values)\n",
    "print(mtdata['EN'].head(2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "48f10124-4c1b-1f09-512d-352c068de1b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mtdata_fr = []\n",
    "for fr in mtdata.FR:\n",
    "    mtdata_fr.append(fr)\n",
    "mtdata_en = []\n",
    "for en in mtdata.EN:\n",
    "    mtdata_en.append(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "3f980247-3c32-240d-7d3d-7b0c3c6c13e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(words_dict, text):\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = 1\n",
    "            else:\n",
    "                words_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "3e9ce130-88f4-8779-5b5f-86f2a23ab347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total French words in Vocabulary: 159523\n",
      "Total English words in Vocabulary 127479\n"
     ]
    }
   ],
   "source": [
    "word_counts_dict_fr = {}\n",
    "word_counts_dict_en = {}\n",
    "count_words(word_counts_dict_fr, mtdata_fr)\n",
    "count_words(word_counts_dict_en, mtdata_en)\n",
    "            \n",
    "print(\"Total French words in Vocabulary:\", len(word_counts_dict_fr))\n",
    "print(\"Total English words in Vocabulary\", len(word_counts_dict_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word_vector_matrix(vector_file):\n",
    "    embedding_index = {}\n",
    "    with codecs.open(vector_file, 'r', 'utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            sr = line.split()\n",
    "            if(len(sr)<26):\n",
    "                continue\n",
    "            word = sr[0]\n",
    "            embedding = np.asarray(sr[1:], dtype='float32')\n",
    "            embedding_index[word] = embedding\n",
    "    return embedding_index\n",
    "embeddings_index = build_word_vector_matrix('/Users/i346047/prs/temp/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "0be42a13-70b2-e9cc-7468-1247b01f109c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word2id_mapping(word_counts_dict):\n",
    "    word2int = {} \n",
    "    count_threshold = 20\n",
    "    value = 0\n",
    "    for word, count in word_counts_dict.items():\n",
    "        if count >= count_threshold or word in embeddings_index:\n",
    "            word2int[word] = value\n",
    "            value += 1\n",
    "\n",
    "\n",
    "    special_codes = [TOKEN_UNK,TOKEN_PAD,TOKEN_EOS,TOKEN_GO]   \n",
    "\n",
    "    for code in special_codes:\n",
    "        word2int[code] = len(word2int)\n",
    "\n",
    "    int2word = {}\n",
    "    for word, value in word2int.items():\n",
    "        int2word[value] = word\n",
    "    return word2int,int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_embeddings(word2int):\n",
    "    embedding_dim = 50\n",
    "    nwords = len(word2int)\n",
    "\n",
    "    word_emb_matrix = np.zeros((nwords, embedding_dim), dtype=np.float32)\n",
    "    for word, i in word2int.items():\n",
    "        if word in embeddings_index:\n",
    "            word_emb_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            word_emb_matrix[i] = new_embedding\n",
    "    return word_emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "4401990d-4baf-3f30-becc-3a6149716b56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of french word embeddings:  19708\n",
      "Length of english word embeddings:  39614\n"
     ]
    }
   ],
   "source": [
    "fr_word2int,fr_int2word = build_word2id_mapping(word_counts_dict_fr)\n",
    "en_word2int,en_int2word = build_word2id_mapping(word_counts_dict_en)\n",
    "fr_embeddings_matrix = build_embeddings(fr_word2int)\n",
    "en_embeddings_matrix = build_embeddings(en_word2int)\n",
    "print(\"Length of french word embeddings: \", len(fr_embeddings_matrix))\n",
    "print(\"Length of english word embeddings: \", len(en_embeddings_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "25cfd0e3-ae3d-8728-1c82-1a61bb06aa0e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_sentence_to_ids(text, word2int, eos=False):\n",
    "    wordints = []\n",
    "    word_count = 0\n",
    "    for sentence in text:\n",
    "        sentence2ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in word2int:\n",
    "                sentence2ints.append(word2int[word])\n",
    "            else:\n",
    "                sentence2ints.append(word2int[TOKEN_UNK])\n",
    "        if eos:\n",
    "            sentence2ints.append(word2int[TOKEN_EOS])\n",
    "        wordints.append(sentence2ints)\n",
    "    return wordints, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "360cfdf4-ad4c-0316-56d3-70b6206e75e4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_fr, word_count_fr = convert_sentence_to_ids(mtdata_fr, fr_word2int)\n",
    "id_en, word_count_en = convert_sentence_to_ids(mtdata_en, en_word2int, eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "2a0ae7cd-a845-23dc-3563-ad133e2f02b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unknown_tokens(sentence, word2int):\n",
    "    unk_token_count = 0\n",
    "    for word in sentence:\n",
    "        if word == word2int[TOKEN_UNK]:\n",
    "            unk_token_count += 1\n",
    "    return unk_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "50d631a2-fb5a-bb0d-6155-9cd15e70835b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of filtered french/english sentences:  200404 200404\n"
     ]
    }
   ],
   "source": [
    "en_filtered = []\n",
    "fr_filtered = []\n",
    "max_en_length = int(mtdata.EN_len.max())\n",
    "max_fr_length = int(mtdata.FR_len.max())\n",
    "min_length = 4\n",
    "unknown_token_en_limit = 10\n",
    "unknown_token_fr_limit = 10\n",
    "\n",
    "for count,text in enumerate(id_en):\n",
    "    unknown_token_en = unknown_tokens(id_en[count],en_word2int)\n",
    "    unknown_token_fr = unknown_tokens(id_fr[count],fr_word2int)\n",
    "    en_len = len(id_en[count])\n",
    "    fr_len = len(id_fr[count])\n",
    "    if( (unknown_token_en>unknown_token_en_limit) or (unknown_token_fr>unknown_token_fr_limit) or \n",
    "       (en_len<min_length) or (fr_len<min_length) ):\n",
    "        continue\n",
    "    fr_filtered.append(id_fr[count])\n",
    "    en_filtered.append(id_en[count])\n",
    "print(\"Length of filtered french/english sentences: \", len(fr_filtered), len(en_filtered) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "34d28c5f-8016-6b36-664e-3d5ee3db745d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    inputs_data = tf.placeholder(tf.int32, [None, None], name='input_data')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    dropout_probs = tf.placeholder(tf.float32, name='dropout_probs')\n",
    "    en_len = tf.placeholder(tf.int32, (None,), name='en_len')\n",
    "    max_en_len = tf.reduce_max(en_len, name='max_en_len')\n",
    "    fr_len = tf.placeholder(tf.int32, (None,), name='fr_len')\n",
    "    return inputs_data, targets, learning_rate, dropout_probs, en_len, max_en_len, fr_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "9c9b6087-3c28-478d-d311-4213e1c59654",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, word2int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    decoding_input = tf.concat([tf.fill([batch_size, 1], word2int[TOKEN_GO]), ending], 1)\n",
    "    return decoding_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "d675562b-a9e0-df71-6979-a052fb78dcbc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rnn_cell(rnn_cell_size,dropout_prob):\n",
    "    rnn_c = GRUCell(rnn_cell_size)\n",
    "    rnn_c = DropoutWrapper(rnn_c, input_keep_prob = dropout_prob)\n",
    "    return rnn_c\n",
    "\n",
    "def encoding_layer(rnn_cell_size, sequence_len, n_layers, rnn_inputs, dropout_prob):\n",
    "    for l in range(n_layers):\n",
    "        with tf.variable_scope('encoding_l_{}'.format(l)):\n",
    "            rnn_fw = get_rnn_cell(rnn_cell_size,dropout_prob)\n",
    "            rnn_bw = get_rnn_cell(rnn_cell_size,dropout_prob)\n",
    "            encoding_output, encoding_state = tf.nn.bidirectional_dynamic_rnn(rnn_fw, rnn_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_len,\n",
    "                                                                    dtype=tf.float32)\n",
    "    encoding_output = tf.concat(encoding_output,2)\n",
    "    return encoding_output, encoding_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "524d0246-ddae-b485-5ea4-11ad476447f4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_decoding_layer(decoding_embed_input, en_len, decoding_cell, initial_state, op_layer, \n",
    "                            v_size, max_en_len):\n",
    "    helper = TrainingHelper(inputs=decoding_embed_input,sequence_length=en_len, time_major=False)\n",
    "    dec = BasicDecoder(decoding_cell,helper,initial_state,op_layer) \n",
    "    logits, _, _ = dynamic_decode(dec,output_time_major=False,impute_finished=True, \n",
    "                                  maximum_iterations=max_en_len)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "6044b206-7f27-5304-4896-06d388af0949",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, decoding_cell, initial_state, op_layer,\n",
    "                             max_en_len, batch_size):\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    inf_helper = GreedyEmbeddingHelper(embeddings,start_tokens,end_token)\n",
    "    inf_decoder = BasicDecoder(decoding_cell,inf_helper,initial_state,op_layer)       \n",
    "    inf_logits, _, _ = dynamic_decode(inf_decoder,output_time_major=False,impute_finished=True,\n",
    "                                                            maximum_iterations=max_en_len)\n",
    "    return inf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "4b50746f-8f78-0253-9178-56c62e4ac1bf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(decoding_embed_inp, embeddings, encoding_op, encoding_st, v_size, fr_len, \n",
    "                   en_len,max_en_len, rnn_cell_size, word2int, dropout_prob, batch_size, n_layers):\n",
    "    \n",
    "    for l in range(n_layers):\n",
    "        with tf.variable_scope('dec_rnn_layer_{}'.format(l)):\n",
    "            gru = tf.contrib.rnn.GRUCell(rnn_len)\n",
    "            decoding_cell = tf.contrib.rnn.DropoutWrapper(gru,input_keep_prob = dropout_prob)\n",
    "    out_l = Dense(v_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attention = BahdanauAttention(rnn_cell_size, encoding_op,fr_len,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "    decoding_cell =  AttentionWrapper(decoding_cell,attention,rnn_len)\n",
    "    attention_zero_state = decoding_cell.zero_state(batch_size , tf.float32 )\n",
    "    attention_zero_state = attention_zero_state.clone(cell_state = encoding_st[0])\n",
    "    with tf.variable_scope(\"decoding_layer\"):\n",
    "        logits_tr = training_decoding_layer(decoding_embed_inp, \n",
    "                                                  en_len, \n",
    "                                                  decoding_cell, \n",
    "                                                  attention_zero_state,\n",
    "                                                  out_l,\n",
    "                                                  v_size, \n",
    "                                                  max_en_len)\n",
    "    with tf.variable_scope(\"decoding_layer\", reuse=True):\n",
    "        logits_inf = inference_decoding_layer(embeddings,  \n",
    "                                                    word2int[TOKEN_GO], \n",
    "                                                    word2int[TOKEN_EOS],\n",
    "                                                    decoding_cell, \n",
    "                                                    attention_zero_state, \n",
    "                                                    out_l,\n",
    "                                                    max_en_len,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return logits_tr, logits_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "19ddcf22-4f6a-d531-071a-021b42b643e3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_en_data, dropout_prob, fr_len, en_len, max_en_len, \n",
    "                  v_size, rnn_cell_size, n_layers, word2int_en, batch_size):\n",
    "    \n",
    "    input_word_embeddings = tf.Variable(fr_embeddings_matrix, name=\"input_word_embeddings\")\n",
    "    encoding_embed_input = tf.nn.embedding_lookup(input_word_embeddings, input_data)\n",
    "    encoding_op, encoding_st = encoding_layer(rnn_cell_size, fr_len, n_layers, encoding_embed_input, dropout_prob)\n",
    "    \n",
    "    decoding_input = process_encoding_input(target_en_data, word2int_en, batch_size)\n",
    "    decoding_embed_input = tf.nn.embedding_lookup(en_embeddings_matrix, decoding_input)\n",
    "    \n",
    "    tr_logits, inf_logits  = decoding_layer(decoding_embed_input, \n",
    "                                                        en_embeddings_matrix,\n",
    "                                                        encoding_op,\n",
    "                                                        encoding_st, \n",
    "                                                        v_size, \n",
    "                                                        fr_len, \n",
    "                                                        en_len, \n",
    "                                                        max_en_len,\n",
    "                                                        rnn_cell_size, \n",
    "                                                        word2int_en, \n",
    "                                                        dropout_prob, \n",
    "                                                        batch_size,\n",
    "                                                        n_layers)\n",
    "    \n",
    "    return tr_logits, inf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "725e92bf-2309-1a78-c771-641a42b440c6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentences(sentences_batch,word2int):\n",
    "    max_sentence = max([len(sentence) for sentence in sentences_batch])\n",
    "    return [sentence + [word2int[TOKEN_PAD]] * (max_sentence - len(sentence)) for sentence in sentences_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "47e4f70a-6377-68dd-c06c-eed674b2bb3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(en_text, fr_text, batch_size):\n",
    "    for batch_idx in range(0, len(fr_text)//batch_size):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        en_batch = en_text[start_idx:start_idx + batch_size]\n",
    "        fr_batch = fr_text[start_idx:start_idx + batch_size]\n",
    "        pad_en_batch = np.array(pad_sentences(en_batch, en_word2int))\n",
    "        pad_fr_batch = np.array(pad_sentences(fr_batch,fr_word2int))\n",
    "\n",
    "        pad_en_lens = []\n",
    "        for en_b in pad_en_batch:\n",
    "            pad_en_lens.append(len(en_b))\n",
    "        \n",
    "        pad_fr_lens = []\n",
    "        for fr_b in pad_fr_batch:\n",
    "            pad_fr_lens.append(len(fr_b))\n",
    "        \n",
    "        yield pad_en_batch, pad_fr_batch, pad_en_lens, pad_fr_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "77299c4b-a3cf-785b-981a-42a1bb3a2033",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "rnn_len = 256\n",
    "n_layers = 2\n",
    "lr = 0.005\n",
    "dr_prob = 0.75\n",
    "logs_path='/tmp/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "68781626-8bf4-0a23-4bb2-f24a5762fa1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph created.\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    input_data, targets, learning_rate, dropout_probs, en_len, max_en_len, fr_len = model_inputs()\n",
    "\n",
    "    logits_tr, logits_inf = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      dropout_probs,   \n",
    "                                                      fr_len,\n",
    "                                                      en_len,\n",
    "                                                      max_en_len,\n",
    "                                                      len(en_word2int)+1,\n",
    "                                                      rnn_len, \n",
    "                                                      n_layers, \n",
    "                                                      en_word2int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    logits_tr = tf.identity(logits_tr.rnn_output, 'logits_tr')\n",
    "    logits_inf = tf.identity(logits_inf.sample_id, name='predictions')\n",
    "    \n",
    "    seq_masks = tf.sequence_mask(en_len, max_en_len, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        tr_cost = sequence_loss(logits_tr,targets,seq_masks)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        gradients = optimizer.compute_gradients(tr_cost)\n",
    "        capped_gradients = [(tf.clip_by_value(gradient, -5., 5.), var) for gradient, var in gradients \n",
    "                        if gradient is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    tf.summary.scalar(\"cost\", tr_cost)\n",
    "print(\"Graph created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "6368ba0d-4083-e182-ca38-5356b307e09f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Epoch   1/20 Batch   20/3131 - Batch Loss:  6.496, seconds: 202.15\n",
      "** Epoch   1/20 Batch   40/3131 - Batch Loss:  2.375, seconds: 172.72\n",
      "** Epoch   1/20 Batch   60/3131 - Batch Loss:  2.385, seconds: 162.24\n",
      "** Epoch   1/20 Batch   80/3131 - Batch Loss:  2.027, seconds: 226.47\n",
      "** Epoch   1/20 Batch  100/3131 - Batch Loss:  2.150, seconds: 163.36\n",
      "** Epoch   1/20 Batch  120/3131 - Batch Loss:  2.199, seconds: 250.74\n",
      "** Epoch   1/20 Batch  140/3131 - Batch Loss:  2.386, seconds: 197.10\n",
      "Average loss: 2.794\n",
      "Saving model\n",
      "** Epoch   1/20 Batch  160/3131 - Batch Loss:  2.235, seconds: 178.15\n",
      "** Epoch   1/20 Batch  180/3131 - Batch Loss:  2.035, seconds: 155.11\n",
      "** Epoch   1/20 Batch  200/3131 - Batch Loss:  2.188, seconds: 242.72\n",
      "** Epoch   1/20 Batch  220/3131 - Batch Loss:  2.133, seconds: 126.80\n",
      "** Epoch   1/20 Batch  240/3131 - Batch Loss:  2.030, seconds: 209.44\n",
      "** Epoch   1/20 Batch  260/3131 - Batch Loss:  1.966, seconds: 401.05\n",
      "** Epoch   1/20 Batch  280/3131 - Batch Loss:  2.032, seconds: 169.62\n",
      "** Epoch   1/20 Batch  300/3131 - Batch Loss:  1.924, seconds: 186.15\n",
      "Average loss: 2.064\n",
      "Saving model\n",
      "** Epoch   1/20 Batch  320/3131 - Batch Loss:  1.996, seconds: 318.40\n",
      "** Epoch   1/20 Batch  340/3131 - Batch Loss:  2.102, seconds: 245.58\n",
      "** Epoch   1/20 Batch  360/3131 - Batch Loss:  2.020, seconds: 132.88\n",
      "** Epoch   1/20 Batch  380/3131 - Batch Loss:  1.906, seconds: 202.28\n",
      "** Epoch   1/20 Batch  400/3131 - Batch Loss:  1.918, seconds: 180.16\n",
      "** Epoch   1/20 Batch  420/3131 - Batch Loss:  2.027, seconds: 122.86\n",
      "** Epoch   1/20 Batch  440/3131 - Batch Loss:  1.833, seconds: 175.38\n",
      "** Epoch   1/20 Batch  460/3131 - Batch Loss:  1.984, seconds: 149.05\n",
      "Average loss: 1.957\n",
      "Saving model\n",
      "** Epoch   2/20 Batch   20/3131 - Batch Loss:  1.977, seconds: 207.10\n",
      "** Epoch   2/20 Batch   40/3131 - Batch Loss:  1.786, seconds: 171.05\n",
      "** Epoch   2/20 Batch   60/3131 - Batch Loss:  1.884, seconds: 162.28\n",
      "** Epoch   2/20 Batch   80/3131 - Batch Loss:  1.624, seconds: 227.77\n",
      "** Epoch   2/20 Batch  100/3131 - Batch Loss:  1.755, seconds: 163.90\n",
      "** Epoch   2/20 Batch  120/3131 - Batch Loss:  1.749, seconds: 249.74\n",
      "** Epoch   2/20 Batch  140/3131 - Batch Loss:  1.921, seconds: 196.67\n",
      "Average loss: 1.808\n",
      "Saving model\n",
      "** Epoch   2/20 Batch  160/3131 - Batch Loss:  1.799, seconds: 173.86\n",
      "** Epoch   2/20 Batch  180/3131 - Batch Loss:  1.633, seconds: 155.68\n",
      "** Epoch   2/20 Batch  200/3131 - Batch Loss:  1.782, seconds: 235.51\n",
      "** Epoch   2/20 Batch  220/3131 - Batch Loss:  1.742, seconds: 125.46\n",
      "** Epoch   2/20 Batch  240/3131 - Batch Loss:  1.652, seconds: 203.35\n",
      "** Epoch   2/20 Batch  260/3131 - Batch Loss:  1.652, seconds: 384.34\n",
      "** Epoch   2/20 Batch  280/3131 - Batch Loss:  1.697, seconds: 169.16\n",
      "** Epoch   2/20 Batch  300/3131 - Batch Loss:  1.572, seconds: 176.49\n",
      "Average loss: 1.69\n",
      "Saving model\n",
      "** Epoch   2/20 Batch  320/3131 - Batch Loss:  1.622, seconds: 312.43\n",
      "** Epoch   2/20 Batch  340/3131 - Batch Loss:  1.741, seconds: 240.91\n",
      "** Epoch   2/20 Batch  360/3131 - Batch Loss:  1.637, seconds: 131.63\n",
      "** Epoch   2/20 Batch  380/3131 - Batch Loss:  1.532, seconds: 201.46\n",
      "** Epoch   2/20 Batch  400/3131 - Batch Loss:  1.601, seconds: 180.03\n",
      "** Epoch   2/20 Batch  420/3131 - Batch Loss:  1.680, seconds: 123.66\n",
      "** Epoch   2/20 Batch  440/3131 - Batch Loss:  1.485, seconds: 174.01\n",
      "** Epoch   2/20 Batch  460/3131 - Batch Loss:  1.627, seconds: 151.40\n",
      "Average loss: 1.606\n",
      "Saving model\n",
      "** Epoch   3/20 Batch   20/3131 - Batch Loss:  1.653, seconds: 207.80\n",
      "** Epoch   3/20 Batch   40/3131 - Batch Loss:  1.502, seconds: 173.10\n",
      "** Epoch   3/20 Batch   60/3131 - Batch Loss:  1.614, seconds: 163.37\n",
      "** Epoch   3/20 Batch   80/3131 - Batch Loss:  1.383, seconds: 226.92\n",
      "** Epoch   3/20 Batch  100/3131 - Batch Loss:  1.434, seconds: 164.66\n",
      "** Epoch   3/20 Batch  120/3131 - Batch Loss:  1.438, seconds: 250.48\n",
      "** Epoch   3/20 Batch  140/3131 - Batch Loss:  1.656, seconds: 195.25\n",
      "Average loss: 1.525\n",
      "Saving model\n",
      "** Epoch   3/20 Batch  160/3131 - Batch Loss:  1.541, seconds: 176.36\n",
      "** Epoch   3/20 Batch  180/3131 - Batch Loss:  1.389, seconds: 153.87\n",
      "** Epoch   3/20 Batch  200/3131 - Batch Loss:  1.523, seconds: 236.21\n",
      "** Epoch   3/20 Batch  220/3131 - Batch Loss:  1.498, seconds: 126.80\n",
      "** Epoch   3/20 Batch  240/3131 - Batch Loss:  1.408, seconds: 202.45\n",
      "** Epoch   3/20 Batch  260/3131 - Batch Loss:  1.437, seconds: 382.54\n",
      "** Epoch   3/20 Batch  280/3131 - Batch Loss:  1.447, seconds: 168.07\n",
      "** Epoch   3/20 Batch  300/3131 - Batch Loss:  1.338, seconds: 193.02\n",
      "Average loss: 1.447\n",
      "Saving model\n",
      "** Epoch   3/20 Batch  320/3131 - Batch Loss:  1.414, seconds: 311.83\n",
      "** Epoch   3/20 Batch  340/3131 - Batch Loss:  1.507, seconds: 242.31\n",
      "** Epoch   3/20 Batch  360/3131 - Batch Loss:  1.394, seconds: 129.47\n",
      "** Epoch   3/20 Batch  380/3131 - Batch Loss:  1.276, seconds: 200.68\n",
      "** Epoch   3/20 Batch  400/3131 - Batch Loss:  1.364, seconds: 179.28\n",
      "** Epoch   3/20 Batch  420/3131 - Batch Loss:  1.458, seconds: 122.98\n",
      "** Epoch   3/20 Batch  440/3131 - Batch Loss:  1.295, seconds: 173.00\n",
      "** Epoch   3/20 Batch  460/3131 - Batch Loss:  1.423, seconds: 149.97\n",
      "Average loss: 1.383\n",
      "Saving model\n",
      "** Epoch   4/20 Batch   20/3131 - Batch Loss:  1.446, seconds: 205.73\n",
      "** Epoch   4/20 Batch   40/3131 - Batch Loss:  1.303, seconds: 169.23\n",
      "** Epoch   4/20 Batch   60/3131 - Batch Loss:  1.441, seconds: 162.06\n",
      "** Epoch   4/20 Batch   80/3131 - Batch Loss:  1.220, seconds: 224.27\n",
      "** Epoch   4/20 Batch  100/3131 - Batch Loss:  1.238, seconds: 162.32\n",
      "** Epoch   4/20 Batch  120/3131 - Batch Loss:  1.235, seconds: 245.94\n",
      "** Epoch   4/20 Batch  140/3131 - Batch Loss:  1.437, seconds: 191.39\n",
      "Average loss: 1.334\n",
      "Saving model\n",
      "** Epoch   4/20 Batch  160/3131 - Batch Loss:  1.374, seconds: 172.98\n",
      "** Epoch   4/20 Batch  180/3131 - Batch Loss:  1.228, seconds: 150.76\n",
      "** Epoch   4/20 Batch  200/3131 - Batch Loss:  1.342, seconds: 230.55\n",
      "** Epoch   4/20 Batch  220/3131 - Batch Loss:  1.293, seconds: 121.59\n",
      "** Epoch   4/20 Batch  240/3131 - Batch Loss:  1.244, seconds: 198.69\n",
      "** Epoch   4/20 Batch  260/3131 - Batch Loss:  1.298, seconds: 378.75\n",
      "** Epoch   4/20 Batch  280/3131 - Batch Loss:  1.288, seconds: 163.30\n",
      "** Epoch   4/20 Batch  300/3131 - Batch Loss:  1.209, seconds: 171.87\n",
      "Average loss: 1.28\n",
      "Saving model\n",
      "** Epoch   4/20 Batch  320/3131 - Batch Loss:  1.214, seconds: 303.62\n",
      "** Epoch   4/20 Batch  340/3131 - Batch Loss:  1.344, seconds: 234.72\n",
      "** Epoch   4/20 Batch  360/3131 - Batch Loss:  1.257, seconds: 127.17\n",
      "** Epoch   4/20 Batch  380/3131 - Batch Loss:  1.130, seconds: 196.06\n",
      "** Epoch   4/20 Batch  400/3131 - Batch Loss:  1.227, seconds: 175.07\n",
      "** Epoch   4/20 Batch  420/3131 - Batch Loss:  1.335, seconds: 120.45\n",
      "** Epoch   4/20 Batch  440/3131 - Batch Loss:  1.155, seconds: 170.29\n",
      "** Epoch   4/20 Batch  460/3131 - Batch Loss:  1.253, seconds: 146.73\n",
      "Average loss: 1.237\n",
      "Saving model\n",
      "** Epoch   5/20 Batch   20/3131 - Batch Loss:  1.279, seconds: 200.91\n",
      "** Epoch   5/20 Batch   40/3131 - Batch Loss:  1.152, seconds: 168.11\n",
      "** Epoch   5/20 Batch   60/3131 - Batch Loss:  1.280, seconds: 158.37\n",
      "** Epoch   5/20 Batch   80/3131 - Batch Loss:  1.119, seconds: 220.32\n",
      "** Epoch   5/20 Batch  100/3131 - Batch Loss:  1.114, seconds: 158.66\n",
      "** Epoch   5/20 Batch  120/3131 - Batch Loss:  1.099, seconds: 243.36\n",
      "** Epoch   5/20 Batch  140/3131 - Batch Loss:  1.276, seconds: 191.05\n",
      "Average loss: 1.189\n",
      "Saving model\n",
      "** Epoch   5/20 Batch  160/3131 - Batch Loss:  1.207, seconds: 170.20\n",
      "** Epoch   5/20 Batch  180/3131 - Batch Loss:  1.108, seconds: 152.18\n",
      "** Epoch   5/20 Batch  200/3131 - Batch Loss:  1.222, seconds: 231.40\n",
      "** Epoch   5/20 Batch  220/3131 - Batch Loss:  1.198, seconds: 122.92\n",
      "** Epoch   5/20 Batch  240/3131 - Batch Loss:  1.163, seconds: 200.05\n",
      "** Epoch   5/20 Batch  260/3131 - Batch Loss:  1.197, seconds: 377.77\n",
      "** Epoch   5/20 Batch  280/3131 - Batch Loss:  1.202, seconds: 163.02\n",
      "** Epoch   5/20 Batch  300/3131 - Batch Loss:  1.107, seconds: 176.45\n",
      "Average loss: 1.177\n",
      "Saving model\n",
      "** Epoch   5/20 Batch  320/3131 - Batch Loss:  1.126, seconds: 307.41\n",
      "** Epoch   5/20 Batch  340/3131 - Batch Loss:  1.255, seconds: 235.98\n",
      "** Epoch   5/20 Batch  360/3131 - Batch Loss:  1.158, seconds: 127.82\n",
      "** Epoch   5/20 Batch  380/3131 - Batch Loss:  1.058, seconds: 194.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Epoch   5/20 Batch  400/3131 - Batch Loss:  1.152, seconds: 175.47\n",
      "** Epoch   5/20 Batch  420/3131 - Batch Loss:  1.191, seconds: 119.74\n",
      "** Epoch   5/20 Batch  440/3131 - Batch Loss:  1.038, seconds: 170.97\n",
      "** Epoch   5/20 Batch  460/3131 - Batch Loss:  1.154, seconds: 147.05\n",
      "Average loss: 1.139\n",
      "Saving model\n",
      "** Epoch   6/20 Batch   20/3131 - Batch Loss:  1.164, seconds: 211.04\n",
      "** Epoch   6/20 Batch   40/3131 - Batch Loss:  1.080, seconds: 168.26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0f80212a518d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                  \u001b[0men_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0men_text_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                  \u001b[0mfr_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfr_text_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                  dropout_probs: dr_prob})\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mupdate_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_learning_rate = 0.0006\n",
    "display_step = 20 \n",
    "stop_early_count = 0 \n",
    "stop_early_max_count = 3 \n",
    "per_epoch = 3 \n",
    "\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] \n",
    "\n",
    "en_train = en_filtered[0:30000]\n",
    "fr_train = fr_filtered[0:30000]\n",
    "update_check = (len(fr_train)//batch_size//per_epoch)-1\n",
    "checkpoint = logs_path + 'best_so_far_model.ckpt' \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    tf_summary_writer = tf.summary.FileWriter(logs_path, graph=train_graph)\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (en_batch, fr_batch, en_text_len, fr_text_len) in enumerate(\n",
    "                get_batches(en_train, fr_train, batch_size)):\n",
    "            before = time.time()\n",
    "            _,loss,summary = sess.run(\n",
    "                [train_op, tr_cost,merged_summary_op],\n",
    "                {input_data: fr_batch,\n",
    "                 targets: en_batch,\n",
    "                 learning_rate: lr,\n",
    "                 en_len: en_text_len,\n",
    "                 fr_len: fr_text_len,\n",
    "                 dropout_probs: dr_prob})\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            after = time.time()\n",
    "            batch_time = after - before\n",
    "            tf_summary_writer.add_summary(summary, epoch_i * batch_size + batch_i)\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('** Epoch {:>3}/{} Batch {:>4}/{} - Batch Loss: {:>6.3f}, seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(fr_filtered) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('Saving model') \n",
    "                    stop_early_count = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early_count += 1\n",
    "                    if stop_early_count == stop_early_max_count:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "\n",
    "        if stop_early_count == stop_early_max_count:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "265fd2f2-cd5f-590d-1fa3-cf6eeedf89fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/models/best_so_far_model.ckpt\n",
      "\n",
      "French Text\n",
      "  Word Ids:    [5, 16, 3171, 136, 35, 19704, 24, 12, 3126, 19704, 95, 19704, 83, 4908, 19704, 14, 19704]\n",
      "  Input Words: Nous avons enlevé tout le <UNK> et la peinture <UNK> qui <UNK> cette fantastique <UNK> en <UNK>\n",
      "\n",
      "English Text\n",
      "  Word Ids:       [140, 932, 43, 43, 43, 14, 39610, 29, 14, 15036, 15036, 37, 3318, 83, 3678, 464, 37]\n",
      "  Response Words: We forgot all all all the <UNK> and the sculpture sculpture that falls this fantastic thing that\n",
      " Ground Truth: We stripped out all the vinyl and <UNK> paint that was covering up this just fantastic aluminum <UNK> <EOS>\n"
     ]
    }
   ],
   "source": [
    "#random = np.random.randint(3000,len(fr_filtered))\n",
    "random = np.random.randint(0,3000)\n",
    "fr_text = fr_filtered[random]\n",
    "\n",
    "checkpoint = logs_path + 'best_so_far_model.ckpt'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    input_data = loaded_graph.get_tensor_by_name('input_data:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    fr_length = loaded_graph.get_tensor_by_name('fr_len:0')\n",
    "    en_length = loaded_graph.get_tensor_by_name('en_len:0')\n",
    "    dropout_prob = loaded_graph.get_tensor_by_name('dropout_probs:0')\n",
    "    result_logits = sess.run(logits, {input_data: [fr_text]*batch_size, \n",
    "                                      en_length: [len(fr_text)], \n",
    "                                      fr_length: [len(fr_text)]*batch_size,\n",
    "                                      dropout_prob: 1.0})[0] \n",
    "\n",
    "pad = en_word2int[TOKEN_PAD] \n",
    "\n",
    "#print('\\nOriginal Text:', input_sentence)\n",
    "\n",
    "print('\\nFrench Text')\n",
    "print('  Word Ids:    {}'.format([i for i in fr_text]))\n",
    "print('  Input Words: {}'.format(\" \".join( [fr_int2word[i] for i in fr_text ] )))\n",
    "\n",
    "print('\\nEnglish Text')\n",
    "print('  Word Ids:       {}'.format([i for i in result_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join( [en_int2word[i]for i in result_logits if i!=pad] )))\n",
    "print(' Ground Truth: {}'.format(\" \".join( [en_int2word[i] for i in en_filtered[random]] )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Voici'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_int2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
